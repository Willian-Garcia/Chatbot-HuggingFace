# NÃO COMITAR .env (use .env.example no repo)
# Hugging Face (opcional — só se quiser geração via LLM)
HF_TOKEN=YOUR_HF_TOKEN_HERE
HF_MODEL=meta-llama/Llama-3.2-1B-Instruct

# Caminhos de contexto e índice
CONTEXT_PATH=context.txt
FAISS_INDEX_PATH=faiss_index.bin

# Modelo de embeddings
EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Configuração de chunking (para generate_cache.py e /ingest)
CHUNK_SIZE=600
CHUNK_OVERLAP=100
EMBED_BATCH_SIZE=64

# Normalização de texto
NORMALIZE_LEVEL=light     # opções: none | light | aggressive
STRIP_DIACRITICS=0        # 1 = remover acentos; 0 = manter

# (Opcional, não usado agora)
ONNX_MODEL_PATH=t5_small.onnx